{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e847431f-cd4b-4eb7-bf45-7b36830e296a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMPORTANT: This data augmentation framework is applied with the assumption that there\n",
    "are only 6 base data augmentations. If this number changes, the framework has to be modified accordingly!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Load the necessary packages:\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import os\n",
    "import random\n",
    "from matplotlib import pyplot as plt, cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from pandas_path import path\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the home directory:\n",
    "home_dir = '/home/jupyter'\n",
    "\n",
    "# Define the maximum number of columns in the dataframes that will be used:\n",
    "pd.set_option(\"max_colwidth\", 80)\n",
    "\n",
    "# Set the random seed for reproducibility:\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "# Functions to preprocess data signals:\n",
    "def drop_frac_and_He(df):\n",
    "    \"\"\"\n",
    "    Drops fractional m/z values, m/z values > 100, and carrier gas m/z\n",
    "\n",
    "    Args:\n",
    "        df: a dataframe representing a single sample, containing m/z values\n",
    "\n",
    "    Returns:\n",
    "        The dataframe without fractional an carrier gas m/z\n",
    "    \"\"\"\n",
    "\n",
    "    # drop fractional m/z values\n",
    "    df = df[df[\"m/z\"].transform(round) == df[\"m/z\"]]\n",
    "    assert df[\"m/z\"].apply(float.is_integer).all(), \"not all m/z are integers\"\n",
    "\n",
    "    # drop m/z values greater than 99\n",
    "    # df = df[df[\"m/z\"] < 100]  # default\n",
    "    \n",
    "    # # drop m/z values greater than 150\n",
    "    df = df[df[\"m/z\"] < 151]\n",
    "\n",
    "    # drop carrier gas\n",
    "    df = df[df[\"m/z\"] != 4]  # default\n",
    "    \n",
    "    # # drop m/z values less than 10\n",
    "    # df = df[df[\"m/z\"] >= 4]     # creates trouble!\n",
    "\n",
    "    return df\n",
    "\n",
    "def remove_background_abundance_pds(df):\n",
    "    \"\"\"\n",
    "    A dummy function for convenience\n",
    "    \n",
    "    \"\"\"\n",
    "    # Modified:\n",
    "    df[\"abun_minsub\"] = df[\"abundance\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Combine all the preprocessing functions:\n",
    "def preprocess_sample(df):\n",
    "    df = drop_frac_and_He(df)\n",
    "    df = remove_background_abundance_pds(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Functions to generate artificial spectra:\n",
    "    Contains 6 base data augmentations\n",
    "    Dervied from the script: generateArtificialSpectra.py\n",
    "\n",
    "    Written by Sabrina Do\n",
    "    NASA GSFC Summer Internship 2023\n",
    "    \n",
    "    Modified by Pugazhenthi Sivasankar,\n",
    "    FDL 2024, SETI Institute, CA.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def shift(input_array, degree_shift):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        * input_array: 1D array\n",
    "        * degree_shift: int, (number between +/- 3), if +, shifts to the right\n",
    "    output: \n",
    "        * 1D array that is shifted\n",
    "    \"\"\"\n",
    "    degree_shift = min(degree_shift, 3)\n",
    "    degree_shift = max(degree_shift, -3)\n",
    "    \n",
    "    return np.roll(input_array, degree_shift)\n",
    "\n",
    "def shiftRandom(input_array, degree_shift, percent_random_peaks_to_shift):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        * input_array: 1D array\n",
    "        * degree_shift: int, (number between +/- 3), if +, shifts to the right\n",
    "    output: \n",
    "        * 1D array that is shifted\n",
    "    \"\"\"\n",
    "\n",
    "    #### Victoria's notes ####\n",
    "    # for this shiftRandom we want to shift some randomly chosen peaks. not all the peaks will be shifted.\n",
    "    # We will have a parameter (percent_random_peaks_to_shift) that will represent the percentage of peaks to shift\n",
    "    # these peaks will be randomly selected and they will be shifted by \"degree_shift\" (maximum is 3)\n",
    "    # The shift should be always done from the original data (the true data) and not from previously generated data\n",
    "\n",
    "    degree_shift = min(degree_shift, 3)\n",
    "    degree_shift = max(degree_shift, -3)\n",
    "\n",
    "    peak_indices = np.nonzero(input_array)[0]\n",
    "\n",
    "    num_peaks_shift = int(len(peak_indices) * (percent_random_peaks_to_shift/100))\n",
    "\n",
    "    indicesToShift = np.random.choice(peak_indices, size=num_peaks_shift, replace=False)\n",
    "\n",
    "    shifted_array = input_array.copy()\n",
    "    array_length = len(shifted_array)\n",
    "\n",
    "    for index in indicesToShift:\n",
    "        new_index = (index + degree_shift) % array_length\n",
    "        shifted_array[new_index] = input_array[index]\n",
    "        shifted_array[index] = 0\n",
    "\n",
    "    \n",
    "    return shifted_array\n",
    "\n",
    "def randomizeIntensity(input_array, percentage_intensity):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        * input_array: 1D array\n",
    "        * percentage_intensity: int of the percentage to increase or decrease\n",
    "    output:\n",
    "        * 1D array with intensity increased\n",
    "    \"\"\"\n",
    "    # probability = 0.5   # Default\n",
    "    probability = 0.7\n",
    "    for i in range(len(input_array)):\n",
    "        if random.random() < probability:\n",
    "            random_factor = random.uniform(0, percentage_intensity)\n",
    "            input_array[i] *= (percentage_intensity / 100 + 1)   # Default\n",
    "            # input_array[i] *= (random_factor / 100 + 1)\n",
    "            \n",
    "    return input_array\n",
    "\n",
    "def add_noise_arushi(input_array, noise_level, type):\n",
    "    '''\n",
    "    Function that adds gaussian or uniform noise to the data. \n",
    "    '''\n",
    "    max_value = max(input_array)\n",
    "    len_noise = len(input_array)\n",
    "\n",
    "    if (type == 'gaussian'):\n",
    "        noise_function = np.array([random.gauss(0, max_value) for i in range(len_noise)])*noise_level\n",
    "    \n",
    "    elif (type == 'white'):\n",
    "        noise_function = np.array([random.uniform(-max_value, max_value) for i in range(len_noise)])*noise_level\n",
    "    \n",
    "    # Include the generated noise:\n",
    "    modified_amp = input_array + noise_function\n",
    "    \n",
    "    return modified_amp\n",
    "\n",
    "\n",
    "def stretch2(input_array, value_stretch_param):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        * input_array: 1D array\n",
    "        * value_stretch_param: how much you want it to be stretched\n",
    "    output:\n",
    "        * 1D array\n",
    "    \n",
    "    function: \n",
    "        * keeps highest peak, stretches on each side of the peak\n",
    "        \n",
    "    Author:\n",
    "        * Pugazhenthi Sivasankar, FDL 2024, SETI Institute, CA\n",
    "    \"\"\"\n",
    "    \n",
    "    # Finding the index of the peak value:\n",
    "    highest_val_index = np.argmax(input_array)\n",
    "    \n",
    "    # Define the percentage by which the segments to the left and right side of the peak\n",
    "    # will increase in value:\n",
    "    percent_increase = 30\n",
    "    \n",
    "    # Check the locations of the highest value index and perform the stretching accordingly:\n",
    "    if (highest_val_index > value_stretch_param) and (highest_val_index < (len(input_array) - value_stretch_param)):\n",
    "\n",
    "\n",
    "        # Increment the left segment:\n",
    "        stretched_values1 = (1 + percent_increase/100)*input_array[:highest_val_index - value_stretch_param]\n",
    "\n",
    "        # Increment the right segment:\n",
    "        stretched_values2 = (1 + percent_increase/100)*input_array[highest_val_index + value_stretch_param:]\n",
    "\n",
    "        # Define the x and y values of the segment to be stretched:\n",
    "        y_exp = np.array([stretched_values1[-1], input_array[highest_val_index], stretched_values2[0]])\n",
    "        x_exp = np.array([highest_val_index - value_stretch_param, highest_val_index,\n",
    "                          highest_val_index + value_stretch_param])\n",
    "\n",
    "        # Stretching the peak to the left using a straight line:\n",
    "        y_inter1 = ((y_exp[1] - y_exp[0])/(x_exp[1] - \n",
    "                                           x_exp[0]))*np.linspace(1, value_stretch_param-1, num=value_stretch_param-1)\n",
    "\n",
    "        # Stretching the peak to the right using a straight line:\n",
    "        y_inter2 = y_exp[1] + (((y_exp[2] - \n",
    "                                 y_exp[1])/(x_exp[2] - x_exp[1]))*\n",
    "                               np.linspace(1, value_stretch_param, num=value_stretch_param))\n",
    "\n",
    "        # Combining all the segments:\n",
    "        stretched_comb = np.append(stretched_values1, y_inter1)\n",
    "        peak_val = np.array(input_array[highest_val_index])\n",
    "        # print('Peak value ' + str(peak_val))\n",
    "        stretched_comb = np.append(stretched_comb, peak_val)\n",
    "        stretched_comb = np.append(stretched_comb, y_inter2)\n",
    "        stretched_comb = np.append(stretched_comb, stretched_values2)\n",
    "\n",
    "    elif (highest_val_index < value_stretch_param):\n",
    "\n",
    "        # Increment the right side:\n",
    "        stretched_right = (1 + percent_increase/100)*input_array[value_stretch_param:]\n",
    "\n",
    "        # Define the x and y values of the segment to be stretched:\n",
    "        # Push the highest value to the start of the array and do a right stretch:\n",
    "        y_exp = np.array([input_array[highest_val_index], stretched_right[0]])\n",
    "        x_exp = np.array([0, value_stretch_param])\n",
    "\n",
    "        # Stretching the peak to the right using a straight line:\n",
    "        y_inter_right = y_exp[0] + (((y_exp[1] - \n",
    "                                 y_exp[0])/(x_exp[1] - x_exp[0]))*\n",
    "                               np.linspace(0, value_stretch_param, num=value_stretch_param))    \n",
    "\n",
    "        # Combining all the segments:\n",
    "        stretched_comb = np.append(y_inter_right, stretched_right)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Increment the left side:\n",
    "        stretched_left = (1 + percent_increase/100)*input_array[:len(input_array) - value_stretch_param]\n",
    "\n",
    "        # Define the x and y values of the segment to be stretched:\n",
    "        # Push the highest value to the end of the array and do a left stretch:\n",
    "        y_exp = np.array([stretched_left[-1], input_array[highest_val_index]])\n",
    "        x_exp = np.array([0, value_stretch_param])  \n",
    "\n",
    "        # Stretching the peak to the left using a straight line:\n",
    "        y_inter_left = y_exp[0] + (((y_exp[1] - \n",
    "                                 y_exp[0])/(x_exp[1] - x_exp[0]))*\n",
    "                               np.linspace(0, value_stretch_param, num=value_stretch_param))    \n",
    "\n",
    "        # Combining all the segments:\n",
    "        stretched_comb = np.append(stretched_left, y_inter_left)    \n",
    "\n",
    "\n",
    "    # if len(stretched_comb) ==  len(input_array):\n",
    "    #     print('Stretched array and input array are equal in length')\n",
    "    # else:\n",
    "    #     print('Unequal array lengths for stretched and input array')\n",
    "\n",
    "    # Return the stretched array:\n",
    "    return stretched_comb\n",
    "\n",
    "\n",
    "def bda_pds_each_spectra_final_norm(df, aug_param, idx):\n",
    "    \"\"\"\n",
    "    Apply 6 base data augmentations to one time slice at a time, and then normalize the\n",
    "    entire array.\n",
    "\n",
    "    Args:\n",
    "        df: dataframe with preprocessed abundance signals\n",
    "        aug_param: a dictionary of data augmentation parameters\n",
    "        idx_total: a list of indices at which the spectra starts in the sample dataframe\n",
    "\n",
    "    Returns:\n",
    "        dataframe with 7 additional columns (1 normalized preprocessed abundance signal\n",
    "        + 6 base augmentations of the preprocessed signal)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a counter to keep track of the number of time slices in a sample:\n",
    "    time_slice_counter = 0\n",
    "\n",
    "    # Initialize empty arrays for concatenation:\n",
    "    y2_norm = np.array([])\n",
    "    y2_shift = np.array([])\n",
    "    y2_shiftRan = np.array([])\n",
    "    y2_randInten = np.array([])\n",
    "    y2_aw_noise = np.array([])\n",
    "    y2_ag_noise = np.array([])\n",
    "    # y2_stretch1 = np.array([])\n",
    "    y2_stretch2 = np.array([])\n",
    "\n",
    "    # Loop over the unique time slices:\n",
    "    for i in range(len(idx) - 1): # To be modified\n",
    "\n",
    "        # Extract the preprocessed signal:\n",
    "        y1 = df[idx[i]:idx[i+1]]['abun_minsub']   # Change \n",
    "\n",
    "        # Concatenate the preprocessed signal:\n",
    "        y2_norm = np.concatenate((y2_norm, y1.values))\n",
    "\n",
    "        # Shift and concatenate the preprocessed signal:\n",
    "        temp_shift = abs(shift(y1.values, aug_param[\"degree_shift\"]))\n",
    "        y2_shift = np.concatenate((y2_shift, temp_shift))\n",
    "\n",
    "        # Randomly shift and concatenate the preprocessed signal:\n",
    "        temp_rand = abs(shiftRandom(y1.values, aug_param[\"degree_shift\"],\n",
    "                                    aug_param[\"percent_random_peaks_to_shift\"]))  \n",
    "        y2_shiftRan = np.concatenate((y2_shiftRan, temp_rand))\n",
    "\n",
    "        # Randomize the intensity and concatenate the preprocessed signal:\n",
    "        temp_randInten = abs(randomizeIntensity(y1.values, aug_param[\"percentage_intensity\"]))\n",
    "        y2_randInten = np.concatenate((y2_randInten, temp_randInten))\n",
    "\n",
    "        # Add white noise and concatenate the preprocessed signal: \n",
    "        temp_aw_noise = abs(add_noise_arushi(y1.values, aug_param[\"arushi_noise_level\"], 'white'))\n",
    "        y2_aw_noise = np.concatenate((y2_aw_noise, temp_aw_noise))  \n",
    "\n",
    "        # Add Gaussian noise and concatenate the preprocessed signal:\n",
    "        temp_ag_noise = abs(add_noise_arushi(y1.values, aug_param[\"arushi_noise_level\"], 'gaussian'))\n",
    "        y2_ag_noise = np.concatenate((y2_ag_noise, temp_ag_noise))    \n",
    "\n",
    "        # Type 2 stretch:\n",
    "        temp_stretch2 = abs(stretch2(y1.values, aug_param[\"value_stretch_param\"]))        \n",
    "        y2_stretch2 = np.concatenate((y2_stretch2, temp_stretch2))          \n",
    "\n",
    "        time_slice_counter += 1\n",
    "\n",
    "    # print(time_slice_counter)\n",
    "\n",
    "    df_aug = df\n",
    "    df_aug[\"abun_ms_norm\"] = y2_norm/max(y2_norm)\n",
    "    df_aug[\"shift\"] = y2_shift/max(y2_shift)\n",
    "    df_aug[\"shiftRan\"] = y2_shiftRan/max(y2_shiftRan)\n",
    "    df_aug[\"randInten\"] = y2_randInten/max(y2_randInten)\n",
    "    df_aug[\"aw_noise\"] = y2_aw_noise/max(y2_aw_noise)\n",
    "    df_aug[\"ag_noise\"] = y2_ag_noise/max(y2_ag_noise)    \n",
    "    df_aug[\"stretch2\"] = y2_stretch2/max(y2_stretch2)\n",
    "\n",
    "    return df_aug\n",
    "\n",
    "def get_total_comb_aug_type(data_aug_base_type):\n",
    "    \n",
    "    # A function that returns a list of combinatorial data augmentation column numbers in the master dataframe.\n",
    "    # Input: a dictionary of the base data augmentation types.\n",
    "    # Output: a list of combinatorial data augmentation columns.\n",
    "\n",
    "    # Function which returns subset or r length from n\n",
    "    from itertools import combinations\n",
    "\n",
    "    # Import math Library\n",
    "    import math \n",
    "\n",
    "    def rSubset(arr, r):\n",
    "\n",
    "        # return list of all subsets of length r\n",
    "        # to deal with duplicate subsets use\n",
    "        # set(list(combinations(arr, r)))\n",
    "        return list(combinations(arr, r))\n",
    "\n",
    "\n",
    "    # Set the number of base data augmentation types:\n",
    "    bda_num = list(data_aug_base_type.keys())\n",
    "\n",
    "    # Number of randomly selected base augmentations:\n",
    "    base_aug_num = [i for i in range(2, len(data_aug_base_type)+1)]\n",
    "\n",
    "    # Initialize a variable to count the number of combinations:\n",
    "    total_comb = 0\n",
    "\n",
    "    # Initialize an empty list:\n",
    "    final_comb_aug_type = []\n",
    "\n",
    "    # Loop over the base data augmentations:\n",
    "    for i in range(len(base_aug_num)):\n",
    "\n",
    "        # Create the combinations:\n",
    "        comb_aug_type = rSubset(bda_num, base_aug_num[i])\n",
    "\n",
    "        # Perform sanity check:\n",
    "        if len(comb_aug_type) == math.comb(len(bda_num), base_aug_num[i]):\n",
    "            print('Number of possible ' + str(base_aug_num[i])\n",
    "                  + ' combinations out of 6 base data augmentations: ' + str(len(comb_aug_type)))\n",
    "\n",
    "        # Horizontally concatenate the combinatorial data augmentations:\n",
    "        final_comb_aug_type = final_comb_aug_type + comb_aug_type\n",
    "\n",
    "        # Increment the counter for combinations:\n",
    "        total_comb += len(comb_aug_type)\n",
    "\n",
    "    # Print the value for verification:\n",
    "    print('Total number of combinatorial augmentations: ' + str(total_comb))\n",
    "    \n",
    "    # Return the list of combinatorial data augmentations:\n",
    "    return final_comb_aug_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f765347c-1e96-4df9-a536-bce37d6a1cc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possible 2 combinations out of 6 base data augmentations: 15\n",
      "Number of possible 3 combinations out of 6 base data augmentations: 20\n",
      "Number of possible 4 combinations out of 6 base data augmentations: 15\n",
      "Number of possible 5 combinations out of 6 base data augmentations: 6\n",
      "Number of possible 6 combinations out of 6 base data augmentations: 1\n",
      "Total number of combinatorial augmentations: 57\n",
      "[(7, 8), (7, 9), (7, 10), (7, 11), (7, 12), (8, 9), (8, 10), (8, 11), (8, 12), (9, 10), (9, 11), (9, 12), (10, 11), (10, 12), (11, 12), (7, 8, 9), (7, 8, 10), (7, 8, 11), (7, 8, 12), (7, 9, 10), (7, 9, 11), (7, 9, 12), (7, 10, 11), (7, 10, 12), (7, 11, 12), (8, 9, 10), (8, 9, 11), (8, 9, 12), (8, 10, 11), (8, 10, 12), (8, 11, 12), (9, 10, 11), (9, 10, 12), (9, 11, 12), (10, 11, 12), (7, 8, 9, 10), (7, 8, 9, 11), (7, 8, 9, 12), (7, 8, 10, 11), (7, 8, 10, 12), (7, 8, 11, 12), (7, 9, 10, 11), (7, 9, 10, 12), (7, 9, 11, 12), (7, 10, 11, 12), (8, 9, 10, 11), (8, 9, 10, 12), (8, 9, 11, 12), (8, 10, 11, 12), (9, 10, 11, 12), (7, 8, 9, 10, 11), (7, 8, 9, 10, 12), (7, 8, 9, 11, 12), (7, 8, 10, 11, 12), (7, 9, 10, 11, 12), (8, 9, 10, 11, 12), (7, 8, 9, 10, 11, 12)]\n"
     ]
    }
   ],
   "source": [
    "# Make a dictionary for the 6 base types of data augmentation:\n",
    "data_aug_base_type = {\n",
    "    7: \"shift\",\n",
    "    8: \"shiftRan\",\n",
    "    9: \"randInten\",\n",
    "    10: \"aw_noise\",\n",
    "    11: \"ag_noise\",\n",
    "    12: \"stretch2\"\n",
    "}\n",
    "\n",
    "# Obtain the various combinations of 6 base data augmentations:\n",
    "final_comb_aug_type = get_total_comb_aug_type(data_aug_base_type)\n",
    "\n",
    "# Print the various combinations of 6 base data augmentations:\n",
    "print(final_comb_aug_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34263533-8954-4094-bd75-3610f7a36a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_comb_data_aug(df_base_aug, final_comb_aug_type):\n",
    "    \n",
    "    # A function that performs combinatorial data augmentations using the 6 base data augmentations.\n",
    "    # NOTE: IF THE NUMBER OF BASE DATA AUGMENTATIONS IS CHANGED FROM 6 TO ANY OTHER NUMBER, THIS\n",
    "    # FUNCTION SHOULD BE MODIFIED ACCORDINGLY!\n",
    "    \n",
    "    # Inputs:\n",
    "    # 1) df_base_aug: a data frame that has the preprocessed and normalized raw signal along with\n",
    "    # the 6 base data augmentations.\n",
    "    # 2) final_comb_aug_type: a list of tuples that contains the combinations of the 6 base augmented \n",
    "    # data columns.\n",
    "    \n",
    "    # Outputs:\n",
    "    # A dictionary that contains 64 data frames in the following format:\n",
    "    # 1st data frame: the preprocessed and normalized raw signal\n",
    "    # 2nd - 7th data frame: the 6 base data augmentations\n",
    "    # 8th - 64th data frame: 57 combinatorial data augmentations.\n",
    "\n",
    "    # Display the first few lines of the desired columns of the DataFrame:\n",
    "    col_interest =  [i for i in range(6, len(df_base_aug.columns))]\n",
    "\n",
    "    # Obtain the eid by splitting the file name:\n",
    "    eid = df_base_aug[\"EID\"][0].split(\".\")[0]\n",
    "\n",
    "    # Get the total number of data augmentations including the \n",
    "    # preprocessed and normalized raw signal:\n",
    "    total_aug_num = 1 + (len(df_base_aug.columns) - 7) + len(final_comb_aug_type)\n",
    "\n",
    "    # Initialize an empty dictionary for collection:\n",
    "    df_coll = {}\n",
    "\n",
    "    # Loop over the total number of augmentations:\n",
    "    for i in range(0, total_aug_num):\n",
    "        if i == 0:\n",
    "\n",
    "            # Store the preprocessed and normalized raw signal:\n",
    "            df_coll[\"df_\"+ eid + \"_\" + \"norm\"] = df_base_aug.iloc[:,[0,1,2,3,4,5,col_interest[i]]]\n",
    "\n",
    "        elif i in [1,2,3,4,5,6]:\n",
    "\n",
    "            # Store the base data augmentations:\n",
    "            df_coll[\"df_\"+ eid + \"_\" + \"base{0}\".format(i)] = df_base_aug.iloc[:,[0,1,2,3,4,5,col_interest[i]]]\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Perform the combinatorial data augmentations:\n",
    "            # Create a dummy dataframe and assign the first 5 columns of the master dataframe:\n",
    "            dummy = df_base_aug.iloc[:,[0,1,2,3,4,5]]\n",
    "\n",
    "            # All possible 2 combinations out of 6 base data augmentations:\n",
    "            if len(final_comb_aug_type[i-7]) == 2:\n",
    "\n",
    "                # Extract the base data augmentations:\n",
    "                y1 = df_base_aug.iloc[:,final_comb_aug_type[i-7][0]]\n",
    "                y2 = df_base_aug.iloc[:,final_comb_aug_type[i-7][1]]\n",
    "\n",
    "                # Create the field name:\n",
    "                combo_name = 'combo_' + \\\n",
    "                str(final_comb_aug_type[i-7][0]) + '_' + \\\n",
    "                str(final_comb_aug_type[i-7][1])\n",
    "\n",
    "                # Find the average of the base data augmentations:\n",
    "                combo_val = np.mean(np.array([y1.values,\n",
    "                                              y2.values]), axis=0)\n",
    "\n",
    "\n",
    "            # All possible 3 combinations out of 6 base data augmentations:\n",
    "            elif len(final_comb_aug_type[i-7]) == 3:\n",
    "\n",
    "                # Extract the base data augmentations:\n",
    "                y1 = df_base_aug.iloc[:,final_comb_aug_type[i-7][0]]\n",
    "                y2 = df_base_aug.iloc[:,final_comb_aug_type[i-7][1]]\n",
    "                y3 = df_base_aug.iloc[:,final_comb_aug_type[i-7][2]]\n",
    "\n",
    "                # Create the field name:\n",
    "                combo_name = 'combo_' + \\\n",
    "                str(final_comb_aug_type[i-7][0]) + '_' + \\\n",
    "                str(final_comb_aug_type[i-7][1]) + '_' + \\\n",
    "                str(final_comb_aug_type[i-7][2])\n",
    "\n",
    "                # Find the average of the base data augmentations:\n",
    "                combo_val = np.mean(np.array([y1.values,\n",
    "                                              y2.values,\n",
    "                                              y3.values]), axis=0)\n",
    "\n",
    "            # All possible 4 combinations out of 6 base data augmentations:\n",
    "            elif len(final_comb_aug_type[i-7]) == 4:\n",
    "\n",
    "                # Extract the base data augmentations:\n",
    "                y1 = df_base_aug.iloc[:,final_comb_aug_type[i-7][0]]\n",
    "                y2 = df_base_aug.iloc[:,final_comb_aug_type[i-7][1]]\n",
    "                y3 = df_base_aug.iloc[:,final_comb_aug_type[i-7][2]]\n",
    "                y4 = df_base_aug.iloc[:,final_comb_aug_type[i-7][3]]\n",
    "\n",
    "                # Create the field name:\n",
    "                combo_name = 'combo_' + \\\n",
    "                str(final_comb_aug_type[i-7][0]) + '_' + \\\n",
    "                str(final_comb_aug_type[i-7][1]) + '_' + \\\n",
    "                str(final_comb_aug_type[i-7][2]) + '_' + \\\n",
    "                str(final_comb_aug_type[i-7][3])\n",
    "\n",
    "                # Find the average of the base data augmentations:\n",
    "                combo_val = np.mean(np.array([y1.values,\n",
    "                                              y2.values,\n",
    "                                              y3.values,\n",
    "                                              y4.values]), axis=0)            \n",
    "\n",
    "\n",
    "            # All possible 5 combinations out of 6 base data augmentations:\n",
    "            elif len(final_comb_aug_type[i-7]) == 5:\n",
    "\n",
    "                # Extract the base data augmentations:\n",
    "                y1 = df_base_aug.iloc[:,final_comb_aug_type[i-7][0]]\n",
    "                y2 = df_base_aug.iloc[:,final_comb_aug_type[i-7][1]]\n",
    "                y3 = df_base_aug.iloc[:,final_comb_aug_type[i-7][2]]\n",
    "                y4 = df_base_aug.iloc[:,final_comb_aug_type[i-7][3]]\n",
    "                y5 = df_base_aug.iloc[:,final_comb_aug_type[i-7][4]]\n",
    "\n",
    "                # Create the field name:\n",
    "                combo_name = 'combo_' + \\\n",
    "                str(final_comb_aug_type[i-7][0]) + '_' + \\\n",
    "                str(final_comb_aug_type[i-7][1]) + '_' + \\\n",
    "                str(final_comb_aug_type[i-7][2]) + '_' + \\\n",
    "                str(final_comb_aug_type[i-7][3]) + '_' + \\\n",
    "                str(final_comb_aug_type[i-7][4])\n",
    "\n",
    "                # Find the average of the base data augmentations:\n",
    "                combo_val = np.mean(np.array([y1.values,\n",
    "                                              y2.values,\n",
    "                                              y3.values,\n",
    "                                              y4.values,\n",
    "                                              y5.values]), axis=0)  \n",
    "\n",
    "            # All possible 6 combinations out of 6 base data augmentations:\n",
    "            elif len(final_comb_aug_type[i-7]) == 6:\n",
    "\n",
    "                # Extract the base data augmentations:\n",
    "                y1 = df_base_aug.iloc[:,final_comb_aug_type[i-7][0]]\n",
    "                y2 = df_base_aug.iloc[:,final_comb_aug_type[i-7][1]]\n",
    "                y3 = df_base_aug.iloc[:,final_comb_aug_type[i-7][2]]\n",
    "                y4 = df_base_aug.iloc[:,final_comb_aug_type[i-7][3]]\n",
    "                y5 = df_base_aug.iloc[:,final_comb_aug_type[i-7][4]]\n",
    "                y6 = df_base_aug.iloc[:,final_comb_aug_type[i-7][5]]\n",
    "\n",
    "                # Create the field name:\n",
    "                combo_name = 'combo_' + \\\n",
    "                str(final_comb_aug_type[i-7][0]) + '_' + \\\n",
    "                str(final_comb_aug_type[i-7][1]) + '_' + \\\n",
    "                str(final_comb_aug_type[i-7][2]) + '_' + \\\n",
    "                str(final_comb_aug_type[i-7][3]) + '_' + \\\n",
    "                str(final_comb_aug_type[i-7][4]) + '_' + \\\n",
    "                str(final_comb_aug_type[i-7][5])\n",
    "\n",
    "                # Find the average of the base data augmentations:\n",
    "                combo_val = np.mean(np.array([y1.values,\n",
    "                                              y2.values,\n",
    "                                              y3.values,\n",
    "                                              y4.values,\n",
    "                                              y5.values,\n",
    "                                              y6.values]), axis=0)\n",
    "                \n",
    "            # Normalize the combinatorial data augmentation:\n",
    "            combo_val_norm = combo_val/max(combo_val)\n",
    "\n",
    "            # Insert the averaged value and the field name as the last column of the dummy dataframe:\n",
    "            dummy.insert(len(dummy.columns), combo_name, combo_val_norm)\n",
    "\n",
    "            # Add the dummy column to dictionary collection:\n",
    "            df_coll[\"df_\"+ eid + \"_\" + \"combo{0}\".format(i-6)] = dummy\n",
    "    \n",
    "    # Return the dictionary of collected data frames:\n",
    "    return df_coll\n",
    "\n",
    "\n",
    "def display_data_aug_pds(df_coll, idx):\n",
    "    \n",
    "    # This function displays a randomly chosen data augmentation at 3 consecutive time slices:\n",
    "\n",
    "    # Input:\n",
    "    # df_coll: A dictionary that contains 64 data frames in the following format:\n",
    "    # 1st data frame: the preprocessed and normalized raw signal\n",
    "    # 2nd - 7th data frame: the 6 base data augmentations\n",
    "    # 8th - 64th data frame: 57 combinatorial data augmentations. \n",
    "    \n",
    "    # Output:\n",
    "    # A figure with 1 x 3 subplots.\n",
    "\n",
    "    # Get the preprocessed and normalized raw signal:\n",
    "    # Ref: https://www.tutorialspoint.com/python-program-to-get-first-and-last-element-from-a-dictionary\n",
    "    norm_key = list(df_coll)[0]\n",
    "    df_norm = df_coll[norm_key]\n",
    "    print(norm_key)\n",
    "    # print(df_norm)\n",
    "\n",
    "    # Get a randomly augmented data:\n",
    "    n_rand = random.randint(1, len(df_coll) - 1)\n",
    "    aug_key_rand = list(df_coll)[n_rand]\n",
    "    df_aug = df_coll[aug_key_rand]\n",
    "    print(aug_key_rand)\n",
    "    # print(df_aug)\n",
    "\n",
    "    # Obtain the EID for display purposes:\n",
    "    chosen_eid = norm_key.split(\"_\")[1]\n",
    "    print(chosen_eid)\n",
    "    mod_label = \"modified: \" + aug_key_rand.split(\"_\")[-1]\n",
    "\n",
    "\n",
    "    # Set the plot layout:\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 6), constrained_layout=True)\n",
    "    fig.suptitle(\"EID:\" + chosen_eid)\n",
    "\n",
    "    for num in range(100, 103):\n",
    "\n",
    "        plt.subplot(1, 3, num-99)\n",
    "        plt.plot(df_norm[idx[num]:idx[num+1]][\"m/z\"],\n",
    "                 df_norm[idx[num]:idx[num+1]].iloc[:,-1], \"-b\", label=\"original\")\n",
    "        plt.plot(df_norm[idx[num]:idx[num+1]][\"m/z\"],\n",
    "                 df_aug[idx[num]:idx[num+1]].iloc[:,-1], \"-r\", label=mod_label)    \n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.grid()\n",
    "        plt.xlim((10,150))\n",
    "        plt.ylim((0,1))\n",
    "        plt.title(\"Spectra: \" + str(num))\n",
    "\n",
    "        \n",
    "def get_sample_df_from_pds_datacube(unique_eid_df):\n",
    "\n",
    "    # This function takes the data frame corresponding to a unique EID from the data cube and transforms it to the data frame format\n",
    "    # required for the data augmentation functions.\n",
    "\n",
    "    # Initialize a loop counter:\n",
    "    loop_counter = 0\n",
    "    \n",
    "    # make spectra index \n",
    "    spectra_idx = np.ones((len(unique_eid_df),), dtype=int)\n",
    "\n",
    "    # Initialize empty arrays for concatenation:\n",
    "    time_array = np.array([])\n",
    "    temp_array = np.array([])\n",
    "    m_over_z_array = np.array([])\n",
    "    abundance_array = np.array([])\n",
    "\n",
    "    for i in range (len(unique_eid_df)):\n",
    "\n",
    "        # Assign the columns to dummy variables:\n",
    "        dummy_time = np.linspace(unique_eid_df['Data'].iloc[i]['time'][0], unique_eid_df['Data'].iloc[i]['time'][-1], \\\n",
    "                                 len((unique_eid_df['Data'].iloc[i]['counts']) ))\n",
    "        dummy_temp = np.linspace(unique_eid_df['Data'].iloc[i]['pryro_temp'][0], unique_eid_df['Data'].iloc[i]['pryro_temp'][-1], \\\n",
    "                                 len((unique_eid_df['Data'].iloc[i]['counts']) ))\n",
    "        dummy_m_over_z = unique_eid_df['Data'].iloc[i]['amu']\n",
    "        dummy_abundance_array = unique_eid_df['Data'].iloc[i]['counts']\n",
    "\n",
    "        # Print the length for clarification:\n",
    "        # print([len(dummy_time), len(dummy_temp), len(dummy_m_over_z), len(dummy_abundance_array)])\n",
    "\n",
    "        # Vertically concatenate the 4 columns:\n",
    "        time_array = np.concatenate((time_array, dummy_time))\n",
    "        temp_array = np.concatenate((temp_array, dummy_temp))\n",
    "        m_over_z_array = np.concatenate((m_over_z_array, dummy_m_over_z))\n",
    "        abundance_array = np.concatenate((abundance_array, dummy_abundance_array))\n",
    "        \n",
    "        # An index to extract spectra:\n",
    "        spectra_idx[i] = (len(time_array))\n",
    "\n",
    "        # Increment the loop counter variable:\n",
    "        loop_counter += 1\n",
    "\n",
    "    # # Display the loop counter value:    \n",
    "    # print(loop_counter)\n",
    "\n",
    "    # Initialize an empty data frame:\n",
    "    df_per_sample = pd.DataFrame()\n",
    "\n",
    "    # Assign the vertically concatenated columns to the data frame columns:\n",
    "    df_per_sample['time'] = time_array\n",
    "    df_per_sample['temp'] = temp_array\n",
    "    df_per_sample['m/z'] = m_over_z_array\n",
    "    df_per_sample['abundance'] = abundance_array\n",
    "\n",
    "    # Return the data frame:\n",
    "    return (df_per_sample, spectra_idx)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a6aa1fd-04a4-413d-b2f5-e1808ae35593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the home directory:\n",
    "os.chdir('/home/jupyter')\n",
    "\n",
    "# Loading the Arushi corrected version:\n",
    "master_pds_df = pd.read_hdf('EGAMS_PDS_Data_AMU;10-150_OverallNorm;Final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc8d206d-02eb-49c3-bd42-9c1b3e5ce3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25048 25257 25297 25306 25327 25333 25350 25366 25484 25493 25500 25505\n",
      " 25515 25530 25538 25565 25572 25579 25596 25635 25644 25653 25660 25669\n",
      " 25695 25705 25719]\n",
      "1332\n",
      "895\n",
      "896\n",
      "897\n",
      "829\n",
      "827\n",
      "827\n",
      "830\n",
      "830\n",
      "829\n",
      "837\n",
      "838\n",
      "771\n",
      "832\n",
      "770\n",
      "832\n",
      "770\n",
      "770\n",
      "736\n",
      "737\n",
      "787\n",
      "787\n",
      "787\n",
      "796\n",
      "773\n",
      "773\n",
      "787\n"
     ]
    }
   ],
   "source": [
    "# Set the main folder path of the data base to save dataframes:\n",
    "main_folder_path = '/home/jupyter/samurai_data_base/'\n",
    "\n",
    "# Define the parameters of the 6 base data augmentations:\n",
    "base_aug_param = {\n",
    "    # \"degree_shift\": 1,\n",
    "    \"degree_shift\": 2,\n",
    "    \"percent_random_peaks_to_shift\": 50,\n",
    "    \"percentage_intensity\": 30,\n",
    "    \"arushi_noise_level\": 0.05,\n",
    "    \"value_stretch_param\": 5\n",
    "}\n",
    "\n",
    "# Find the number of unique sample EIDs\n",
    "unique_eid = master_pds_df['Sample ID'].unique()\n",
    "print(unique_eid)\n",
    "\n",
    "for eid_num in range(0, len(unique_eid)):\n",
    "# for eid_num in range(0, 2):    \n",
    "\n",
    "    # Get all the data points of a given sample:\n",
    "    indx = np.where(master_pds_df['Sample ID'] == unique_eid[eid_num])\n",
    "    unique_eid_df = master_pds_df.iloc[indx]\n",
    "    print(len(unique_eid_df))\n",
    "\n",
    "    # Obtain the necessary data frame from the data cube:\n",
    "    df_per_sample, idx = get_sample_df_from_pds_datacube(unique_eid_df)\n",
    "    \n",
    "    # Remove the background noise:\n",
    "    df_ex_noHe_noBack = remove_background_abundance_pds(df_per_sample)\n",
    "    \n",
    "    # Apply the 6 base data augmentation and finally normalize:\n",
    "    idx_total = np.insert(idx, 0, 0)\n",
    "    df_ex_noHe_noBack_aug = bda_pds_each_spectra_final_norm(df_ex_noHe_noBack, base_aug_param, idx_total)\n",
    "    \n",
    "    # Insert the EID at the beginning of the dataframe:\n",
    "    # (Ref: https://builtin.com/data-science/pandas-add-column)\n",
    "    df_ex_noHe_noBack_aug.insert(0, \"EID\", str(unique_eid[eid_num]) + '.csv')\n",
    "    \n",
    "    # Obtain the combinatorial data augmentations:\n",
    "    df_coll = get_comb_data_aug(df_ex_noHe_noBack_aug, final_comb_aug_type)  \n",
    "    \n",
    "    # # Display the data augmentations:\n",
    "    # display_data_aug_pds(df_coll, idx_total)\n",
    "    \n",
    "    for key_main in df_coll:\n",
    "\n",
    "        # Obtain the folder name:\n",
    "        folder_last_name = key_main.split(\"_\")[-1]\n",
    "        folder_name = main_folder_path + folder_last_name\n",
    "        # print(folder_name)\n",
    "        # print(key_main)\n",
    "\n",
    "        # Change to the suitable directory:\n",
    "        os.chdir(folder_name)\n",
    "\n",
    "        # Extract the DataFrame of the current key:\n",
    "        chosen_df = df_coll[key_main]\n",
    "\n",
    "        # Save the dataframes:\n",
    "        filename = key_main.split(\"_\")[1] + '.hdf'\n",
    "        chosen_df.to_hdf(filename, key='data')    "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m123"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
